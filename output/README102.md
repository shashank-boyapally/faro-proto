# OpenShift Files

## Overview

This folder contains files used for launching the [official TensorFlow High-Performance CNN benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) as an app in OpenShift on AWS. If you wish to build and run TensorFlow on RHEL 8, follow the instructions in the next section carefully. They are required. Otherwise, for RHEL 7 builds, you can skip to the **Basics** section.

## PREP WORK

### 1. Configure

The entire image build, s2i app build, benchmarks, etc. process is streamlined with a Makefile that is generated by the `configure` script in this directory. Before you do anything, run:

```bash
$ ./configure <flags>
```

For help on using `configure`, run:

```bash
$ ./configure --help
```

If you plan on using GPUs and you want to use your AWS s3 bucket to download proprietary NVIDIA packages (that you have uploaded yourself), you must pass in the `--use-aws` flag and create OpenShift secrets for your AWS credentials. Instructions on how to create such secrets will be provided later on.

Note that if you made a mistake somewhere in configuring, just rerun `configure`. Every time you run `configure`, it overwrites the existing Makefile with new information, so it is safe to rerun it.

**HEADS UP:** If you use the `--instance-type`/`-t` flag when configuring the Makefile and specify an instance that does not exist yet (which is perfectly okay!), you will need to setup that instance before executing the `make` commands. Information on how to setup an instance using `MachineSet` will be described later on, but do not worry about that yet. For now, follow the instructions in the next subsection, which describes how to install the necessary cluster operators for running the benchmarks.

### 2. Installing Necessary Cluster Operators

After you've run `configure`, you should setup and install three necessary operators to your cluster: Special Resource Operator (SRO), Node Feature Discovery (NFD) Operator, and Ripsaw. If you haven't installed *any* of these operators, run

```bash
$ make setup_operators
```

The SRO is used for installing NVIDIA drivers to your cluster, the NFD operator is used for discovering node features, and Ripsaw is for running the benchmarks.

If you have installed one or more of these operators already, you can run one or more of these commands to install specific operators:

```bash
$ make setup_gpu     #sets up SRO
$ make setup_nfd     #sets up the NFD operator
$ make setup_ripsaw  #sets up the Ripsaw operator
```

Once you've installed the necessary operators, you should follow the next two subsections for creating 'base images' using registry.redhat.io images. (Note: these sections only apply if you are using RHEL 7 CUDA builds or non-CUDA RHEL 8 builds.)

### 3. Setting up BuildConfig and Benchmark Templates

To build any ImageStream or Benchmark object in this repository, you will need to load the BuildConfig and Benchmark templates into OpenShift. You can do so via:

```bash
$ make -C setup/templates
```

If you wish to make changes to the templates or simply undeploy all of them,

```bash
$ make -C setup/templates undeploy
```

This will perform an `oc delete -f <filename>` command on each template, allowing you to update your templates and -- if you wish -- redeploy them by running the `make -C setup/templates` command.

### 4. Setting up AWS Credentials

To setup your AWS credentials, make sure you've first configured AWS on your own machine. This step is important because the ~/.aws/config and ~/.aws/credentials files are read by a Makefile.

Once you are ready, simply run

```
$ make -C setup/aws_credentials PROFILE="your-desired-configured-profile-name"
```

making sure to replace "your-desired-configured-profile-name" with whichever profile name you have saved in your ~/.aws/config and ~/.aws/credentials

Note that you *may* need sudo access to run this command. In essence, the Makefile sets `$(HOME_DIR)` equal to `${HOME}`. Thus, if your AWS config and credentials files are not in `${HOME}`, you should run as root.

### 5. Using registry.redhat.io Images (REQUIRED FOR NON-CUDA RHEL 8 BUILDS AND *SOME* CUDA RHEL 7 BUILDS)

For non-CUDA RHEL 8 builds, follow the instructions in the `setup` folder in this directory. In essence, you will need to run a create two files under **../../secrets**, then run a `make` command.

If you wish to use the complete set of CUDA packages with your TensorFlow (e.g., CUDA toolkit-devel, TensorRT, etc.), you will need to use one of the custom Dockerfiles described in the next section.

### 6. Building and Using a CUDA "Base Image" from this Repository

To prepare for a CUDA build using a "base image," make sure to build one of the Docker images under `../Dockerfiles/custom/rhel7/cuda` or `../Dockerfiles/custom/rhel8/cuda`. Follow the instructions in the **setup** folder in this directory to do so. Such base images have cuDNN and NCCL pre-installed, which means you can skip the next subsection "Setting up an EBS Volume." Otherwise, for images that do *not* have cuDNN and NCCL pre-installed, you will need to setup an EBS volume that contains such packages by following the instructions below.

### 7. Setting up an EBS Volume (for Non Custom Base Images)

To prepare for CUDA builds in images which do *not* have cuDNN and NCCL preinstalled, you will first need to create an EBS volume, like so:

```bash
$ cd setup/volumes
$ sh create_ebs_volume.sh -n <volume_name> -t <volume_type> -s <volume_size> -z <aws_availability_zone>
```

Once you've created your volume, create a dummy pod that will be used for storing data in the EBS storage via a PV (Persistent Volume):

```bash
$ #cd setup/volumes
$ sh create_temp_nvidia_pod.sh <volume_id>
```

This will create a temporary pod named `tmp-nvidia-pod`, which you can access by executing:

```bash
$ oc exec -it tmp-nvidia-pod -- /bin/bash
```

The EBS volume will be mounted under `/tmp/nvidia_ebs`. From there, you can download your two (required) NVIDIA packages: (1.) NCCL, and (2.) cuDNN.

If you have an s3 bucket where the packages are stored, then install `awscli` via `pip` or `pip3`, configure it to provide your credentials, and download. Otherwise, download from wherever you have your tarballs hosted.

Once you're done, type `exit` to exit the pod. Because you won't be needing it anymore, you can delete it via:

```bash
$ oc delete pod/tmp-nvidia-pod
```

Now you're all set! The EBS volume should have your cuDNN and NCCL tar files!

#### Warning About Using ubi8 and Related Images

The RHEL 8 "ubi8" image has a limited set of packages that can be installed through its included repos. Even NVIDIA's `nvidia/cuda:<tag>` images reference ubi8, so the same issue lies there. Thus, in order to obtain the packages that *cannot* be installed through those repos, you will need to create your own image using one of the custom provided Dockerfiles in `../Dockerfiles/custom` and supply your own `.repo` files to point to RHEL 8 repositories.

## Preparing ImageNet

If you would like to use real ImageNet data, please follow the instructions in **setup/README.md** for ImageNet. This process requires using an EBS volume to 'host' your ImageNet data. It is very similar to the approach of creating a temporary NVIDIA pod for holding NVIDIA packages.

To create an ImageNet EBS volume,

```bash
$ cd setup/volumes
$ sh create_ebs_volume.sh -n <volume_name> -t <volume_type> -s <volume_size> -z <aws_availability_zone>
```

Once you've created your volume, create a dummy pod that will be used for storing data in the EBS storage via a PV (Persistent Volume):

```bash
$ #cd setup/volumes
$ sh create_temp_imagenet_pod.sh <volume_id>
```

From here, enter the pod via 

```bash
oc exec -it tmp-imagenet-pod -- /bin/bash
```

...then download/upload your ImageNet data to the mounted volume located at `/tmp/imagenet_ebs`. Delete the pod when you're done, if desired.

## How to Run the Benchmarks

### Optional Prerequisites

Assuming you have already run `configure` to generate a Makefile, your next steps depend on whether or not you have requested (via `configure`) to use a specific instance type or an instance with specific attributes. If you do want to use a specific instance or specific type of instance, follow the next subsection. Otherwise, you may skip said subsection and move on to **Advanced CPU Options** if you wish to use CPU Manager. You can skip that section too, though, if you are using GPUs or do not want to use CPU Manager.

#### Automatically creating a Node

If you wish to create a MachineSet and run the pod on a node with a specific instance type, use `../../helper_scripts/OpenShift/create_machineset.sh` to create a YAML file. Or you can create your own YAML file. The script is provided as a convenience.

Once your YAML file has been generated,

```bash
$ oc create -f <YAML_filename>
```

If you would like information on how to use the script,

```bash
$ sh ../../create_machineset.sh -h
```

To get your AMI ID and cluster ID, either log into your [AWS console](https://aws.amazon.com/console/) and find your cluster, **or** 

```json
$ aws iam list-instance-profiles --output json | grep <your_cluster_name_or_partial_cluster_name> -B 18
            "InstanceProfileId": "<instance_profile_id>", 
            "Roles": [
                {
                    "AssumeRolePolicyDocument": {
                        "Version": "2012-10-17", 
                        "Statement": [
                            {
                                "Action": "sts:AssumeRole", 
                                "Principal": {
                                    "Service": "ec2.amazonaws.com"
                                }, 
                                "Effect": "Allow", 
                                "Sid": ""
                            }
                        ]
                    }, 
                    "RoleId": "<role_id>", 
                    "CreateDate": "2019-07-18T15:57:33Z", 
                    "RoleName": "<cluster_id>-worker-role", 
                    "Path": "/", 
                    "Arn": "arn:aws:iam:<id>:role/<cluster_id>-worker-role"
                }
            ], 
            "CreateDate": "2019-07-18T15:57:33Z", 
            "InstanceProfileName": "<cluster_id>-worker-profile", 
            "Path": "/", 
            "Arn": "arn:aws:iam::<id>:instance-profile/<cluster_id>-worker-profile"

$ aws ec2 describe-instances --filters "Name=iam-instance-profile.id,Values=<instance_profile_id>" --output json | grep ImageId
                    "ImageId": "ami-<hash>", 
                    "ImageId": "ami-<hash>", 
                    "ImageId": "ami-<hash>", 
```

#### Advanced CPU Options (CPU Manager)

CPU Manager info: https://docs.openshift.com/container-platform/4.1/scalability_and_performance/using-cpu-manager.html

##### Installing and Enabling CPU Manager

First, install and enable CPU Manager to your cluster. To do so,

```bash
$ sh ../../helper_scripts/OpenShift/enable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -x <avx_instruction_set>
```

or

```bash
$ sh ../../helper_scripts/OpenShift/enable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -i <instance_type>
```

##### Uninstalling and Disabling CPU Manager

To uninstall,

```bash
$ sh ../../helper_scripts/OpenShift/disable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -x <avx_instruction_set>
```

or

```bash
$ sh ../../helper_scripts/OpenShift/disable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -i <instance_type>
```

### Basics

#### Running the Benchmarks

Now you're ready to build the necessary images to run the benchmarks. You can do everything at once by running:

```bash
$ make
```

This command will build a "base" imagestream that is used by a Source-to-Image (s2i) strategy that generates an image from the scripts under `../.s2i_fftw` or `../.s2i_openblas`. Once the s2i image has been built, the benchmarks are executed using [Ripsaw](https://github.com/cloud-bulldozer/ripsaw).

To build just the base image stream,

```bash
$ make imagestream
```

To build just the s2i image stream (assuming the base image stream has already been built),

```bash
$ make s2i
```

To run just the benchmarks (assuming both image streams have been built),

```bash
$ make benchmarks
```

As the build commands execute, they will call scripts from the `scripts` folder in this directory. These scripts let you know the status of the build by checking every 10 seconds to see if the build is pending, has succeeded, or has failed. **Do not exit out of these scripts.**  Once the scripts complete, the next `make` target will automatically execute. Or if you are building everything step by step (e.g., `make imagestream`, `make s2i`, etc.), either wait until the referenced script finishes executing before calling your next `make` command, or wait until the build has completed by checking `oc status`.

Note that if you *must* prematurely stop `make imagestream` or `make s2i`, you must run `make clean_imagestream` (if you prematurely stopped `make imagestream`) or `make clean_s2i` (if you prematurely stopped `make s2i`) before running them again. Also, note that even if you *do* run the "clean" commands, your next `make imagestream` or `make s2i` command will likely fail, with an error saying "Image build has STOPPED." This is expected with OpenShift 4.x. To rectify this behavior, simply rerun the `make clean_imagestream` or `make clean_s2i` command (depending on which command failed), followed by `make imagestream` or `make s2i`.

#### "Cleaning" the Benchmarks

The `clean` target in the Makefile deletes: (1.) the base ImageStream object, (2.) the s2i ImageStream object, and (3.) the Ripsaw Benchmark object. Thus, all you need to do is run `make clean` if you wish to delete everything. However, if you wish to delete only *specific* resources,

```bash
$ make clean_imagestream  #Deletes the base image stream
$ make clean_s2i          #Deletes the s2i generated image stream
$ make clean_benchmarks   #Deletes the benchmark object
```

#### Running Benchmarks on a Different Podman/Docker/CRI-O Image

To run the benchmarks on a different image, you will need to rerun `configure` before running your `make` command(s).
